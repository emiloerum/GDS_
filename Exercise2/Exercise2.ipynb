{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\emilo\\miniconda3\\envs\\my_environment\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\emilo\\miniconda3\\envs\\my_environment\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\emilo\\miniconda3\\envs\\my_environment\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\emilo\\miniconda3\\envs\\my_environment\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:40\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_supported_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m ]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes  \u001b[38;5;66;03m# pylint: disable=import-self\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     Resolution,\n\u001b[0;32m     43\u001b[0m     periods_per_day,\n\u001b[0;32m     44\u001b[0m     periods_per_second,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnattype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     NaT,\n\u001b[0;32m     48\u001b[0m     NaTType,\n\u001b[0;32m     49\u001b[0m     iNaT,\n\u001b[0;32m     50\u001b[0m     nat_strings,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mconversion.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:76\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.parsing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "'''For the purpose of not making this pdf version of the notebook many-thousand pages long\n",
    "i have not executed some of the cells. Mainly because the scraping of the webpages on bbc took\n",
    "over 2 hours to complete. As seen by the code the output was saved into a csv file with columns for\n",
    "each key in the result dicts, namely: \"Headline\", \"Author\", \"Published_date\" and \"Text\". ''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1\n",
    "#1\n",
    "#Importing the cleaned dataset from last assignment\n",
    "news_data_cleaned = pd.read_csv('news_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articly types and counts, old:  type\n",
      "fake          155\n",
      "conspiracy     31\n",
      "political      23\n",
      "unreliable      6\n",
      "bias            6\n",
      "junksci         6\n",
      "reliable        3\n",
      "clickbait       1\n",
      "hate            1\n",
      "Name: count, dtype: int64\n",
      "Articly types and counts, new:  type\n",
      "fake        205\n",
      "reliable     39\n",
      "Name: count, dtype: int64\n",
      "Percentage of fake articles in dataset:  84.01639344262296\n",
      "Percentage of fake articles in dataset:  15.983606557377051\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#A\n",
    "#In determining which article types should be omitted we can start\n",
    "#by looking at all the different types:\n",
    "unique_types = news_data_cleaned['type'].unique()\n",
    "# print(unique_types)\n",
    "\n",
    "'''Since we are to make a fake news predicter given a set of labeled data it might\n",
    "not be wise to include the documents where the label is 'unknown' since it\n",
    "maybe indicate that the process that labeled the data in the first place\n",
    "were unable to decide anything for this. Furthermore when reading about the\n",
    "dataset on github we see that articles with the label 'clickbait' generally contain\n",
    "credible news - just with very exaggerated headlines to promote attention, so \n",
    "there is reason to group this together with the reliable data. In continuation of this\n",
    "i have also chosen to include documents with the label 'political' since it is\n",
    "stated on github that these contain \"generally verifiable information in support of\n",
    "certain points of view or political orientation.\" indicating the the information\n",
    "residing in the articles are not fake at all - they just might have some underlying\n",
    "political view but in my opinion that does not constitute fake.'''\n",
    "\n",
    "#B\n",
    "#This all results in the following grouping of 'fake' and 'reliable':\n",
    "fake_news = ['unreliable', 'fake', 'conspiracy', 'bias', 'hate', 'junksci']\n",
    "reliable_news = ['clickbait', 'reliable', 'political']\n",
    "\n",
    "def label_news(label):\n",
    "    '''Function for labeling the data \n",
    "    according to the two new categories:'''\n",
    "    if label in fake_news:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return 'reliable'\n",
    "\n",
    "#Deleting rows where label is 'unknown':\n",
    "news_data_cleaned_filtered = news_data_cleaned[news_data_cleaned['type'] != 'unknown']\n",
    "\n",
    "article_types_old = news_data_cleaned_filtered['type']\n",
    "\n",
    "# Label the type column with the new appropriate label using the label_news function\n",
    "article_types_new = news_data_cleaned_filtered['type'].apply(lambda x: label_news(x))\n",
    "\n",
    "\n",
    "#C\n",
    "# Print value counts for content_column and article_type\n",
    "print('Articly types and counts, old: ', article_types_old.value_counts())\n",
    "print('Articly types and counts, new: ', article_types_new.value_counts())\n",
    "\n",
    "#By our new groupings we end up with 205 articles labeled as fake news and\n",
    "#39 labeled as reliable.\n",
    "fake_percentage = (205/(205+39))*100\n",
    "reliable_percentage =(39/(205+39))*100\n",
    "print('Percentage of fake articles in dataset: ', fake_percentage)\n",
    "print('Percentage of fake articles in dataset: ', reliable_percentage)\n",
    "\n",
    "'''We see that the dataset consists roughly of 84 percent fake news\n",
    "and 16 reliable. Thereby we can say the the dataset is not very balanced.\n",
    "This can be an issue when training models with the data, as the overwhelmingly larger half\n",
    "of fake news in the dataset will lead to biased models, labeling the majority of data fake because its\n",
    "trained almost exclusively on this and failing to label the minority data - in this case\n",
    "the reliable news. This can ultimately lead to bad generalization when introduced to new data.\n",
    "An unbalanced dataset can also lead to skewed evaluation metrics such as accuracy.''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2\n",
    "#2\n",
    "response = requests.get('https://www.bbc.com/news/world/europe')\n",
    "contents = response.text\n",
    "\n",
    "#3\n",
    "soup = BeautifulSoup(contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_links(html_content):\n",
    "    '''function that for each article in some html file retrieves the\n",
    "     corresponding link and returns these links in a list '''\n",
    "    #Creating a bs object containing the input html content parsed with the html.parser.\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    #Find all article tags\n",
    "    articles = soup.find_all('div', {'type' : 'article'})\n",
    "    #Initializing empty list to store links\n",
    "    article_links = []\n",
    "    #Traversing article tags, extracting the link for each article and append it to the list\n",
    "    #initialized above\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        article_links.append(link)\n",
    "    #Return a list of article links from the input\n",
    "    return article_links\n",
    "\n",
    "article_links = extract_article_links(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def extract_all_article_links(url, total_pages):\n",
    "    '''Function for extracting every article link in\n",
    "    each page of \"https://www.bbc.com/news/world/INSERTREGIONHERE\", given some region. \n",
    "    It assumes that one has manually identified the number of total pages of the website\n",
    "    in advance and takes that as an argument as well as the url written above'''\n",
    "    all_article_links = []\n",
    "    #range from 1st site to total_pages + 1 cause the range function doesnt include the last\n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        #create the appropriate url for the given page\n",
    "        page_url = f\"{url}?page={page_num}\"\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #All articles are wrapped in a div - so we find_all divs with the type=article\n",
    "        articles = soup.find_all('div', {'type' : 'article'})\n",
    "        for article in articles:\n",
    "            #Extract all links as with the function \"extract_all_links\" defined above.\n",
    "            link = article.find('a')['href']\n",
    "            all_article_links.append(link)\n",
    "    return all_article_links\n",
    "\n",
    "\n",
    "#Extrating all links from the europe section\n",
    "# all_article_links = extract_all_article_links('https://www.bbc.com/news/world/europe', 42)\n",
    "# print(all_article_links)\n",
    "# #Returns 904 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe: 905 article links found.\n",
      "Asia: 907 article links found.\n",
      "australia: 827 article links found.\n",
      "Africa: 492 article links found.\n",
      "Latin America: 839 article links found.\n",
      "Middle East: 818 article links found.\n",
      "Total article links found: 4788\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "'''Dictionary containing each region of interest. Keys are the names of the regions\n",
    "and values are tuples containing the appropriate url as well as total page count pr region,\n",
    "which i identified manually (by looking at each webpage)'''\n",
    "\n",
    "regions_dict = {\n",
    "    'Europe' : ('https://www.bbc.com/news/world/europe', 42),\n",
    "    'Asia' : ('https://www.bbc.com/news/world/asia', 42),\n",
    "    'australia' : ('https://www.bbc.com/news/world/australia', 42),\n",
    "    'Africa' : ('https://www.bbc.com/news/world/africa', 25),\n",
    "    'Latin America' : ('https://www.bbc.com/news/world/latin_america', 41),\n",
    "    'Middle East' : ('https://www.bbc.com/news/world/middle_east', 41)\n",
    "}\n",
    "\n",
    "def extract_all_region_article_links(regions):\n",
    "    '''Function for extracting all article links from different regions of the\n",
    "    bbc news website. As input the function assumes a dictionary containing the name of the\n",
    "    region as key and more importantly. - The value of each element in the dictionary\n",
    "    should be a tuple containing the appropriate url as well as the total page count'''\n",
    "    all_region_article_links = {}\n",
    "    for region, (url, total_pages) in regions.items():\n",
    "        region_links = extract_all_article_links(url, total_pages)\n",
    "        all_region_article_links[region] = region_links\n",
    "    return all_region_article_links\n",
    "\n",
    "all_region_article_links = extract_all_region_article_links(regions_dict)\n",
    "\n",
    "for region, article_links in all_region_article_links.items():\n",
    "    print(f\"{region}: {len(article_links)} article links found.\")\n",
    "\n",
    "total_links = sum(len(links) for links in all_region_article_links.values())\n",
    "print(f\"Total article links found: {total_links}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "#Creating list containing tuples with all links and their corresponding region\n",
    "region_link_data = []\n",
    "for region, links in all_region_article_links.items():\n",
    "    for link in links:\n",
    "        region_link_data.append((region, link))\n",
    "\n",
    "#Creating dataframe with region, and link columns based on data in list created above\n",
    "df_regionandlink = pd.DataFrame(region_link_data, columns=['Region', 'Link'])\n",
    "\n",
    "'''Uncomment and run the below line to create csv containing all links and their respective regions''';\n",
    "# df_regionandlink.to_csv('region_article_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "#1 and 2\n",
    "def extract_article_information(url):\n",
    "    '''Function for extracting the information of an article given the url to its location\n",
    "    it assumes that the base url is bbc's website and the formal\n",
    "    parameter is the extension of the base and is thus only for scraping articles on\n",
    "    the bbc. It returns a dictionary containing the headline, author, published_date and paragraphs\n",
    "    of the given webpage.'''\n",
    "    try:\n",
    "        response = requests.get('https://www.bbc.com' + url)\n",
    "        contents = response.text\n",
    "        soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "        author_element = soup.find('div', class_=\"ssrcss-68pt20-Text-TextContributorName e8mq1e96\")\n",
    "        #Check if element exists before assigning to author variable to avoid calling get_text() on none object and get an attribute error\n",
    "        author = author_element.get_text() if author_element else None \n",
    "\n",
    "        headline_element = soup.find(id='main-heading')\n",
    "        #Check if element exists before assigning to author variable to avoid calling get_text() on none object and get an attribute error\n",
    "        headline = headline_element.get_text() if headline_element else None\n",
    "\n",
    "        published_date_element = soup.find('time')\n",
    "        # Handle KeyError exception when reaching: /news/live/uk-england-gloucestershire-67611521\n",
    "        # and TypeError when reaching /news/live/world-asia-67605206\n",
    "        try:\n",
    "            published_date = published_date_element['datetime'] if published_date_element else None\n",
    "        except KeyError:\n",
    "            published_date = None\n",
    "\n",
    "        text_ugly = soup.find_all('p', {'class' : 'ssrcss-1q0x1qg-Paragraph e1jhz7w10'})\n",
    "        text_pretty = []\n",
    "        for text in text_ugly:\n",
    "            text_pretty.append(text.get_text())\n",
    "        \n",
    "        #Throwing all the gathered information into dictionary with corresponding keys\n",
    "        article_information_dict = {\n",
    "            'Headline' : headline,\n",
    "            'Author' : author,\n",
    "            'published_date' : published_date,\n",
    "            'text' : text_pretty\n",
    "        }\n",
    "    #Handles exceptions that might occur while fething an article\n",
    "    except requests.exceptions.RequestException as req_e:\n",
    "        print(f'An error occured while fetching the url: {req_e}')\n",
    "        return None\n",
    "\n",
    "    return article_information_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Region                         Link\n",
      "0  Europe     /sport/football/68436828\n",
      "1  Europe  /news/world-europe-68431803\n",
      "2  Europe  /news/world-europe-68423990\n",
      "3  Europe            /news/uk-68429901\n",
      "4  Europe  /news/world-europe-68423229\n",
      "5  Europe  /news/world-europe-68415802\n",
      "6  Europe      /news/business-68401814\n",
      "7  Europe     /sport/football/68417181\n",
      "8  Europe  /news/world-europe-68420566\n",
      "9  Europe  /news/world-europe-68420565\n",
      "0       /sport/football/68436828\n",
      "1    /news/world-europe-68431803\n",
      "2    /news/world-europe-68423990\n",
      "3              /news/uk-68429901\n",
      "4    /news/world-europe-68423229\n",
      "5    /news/world-europe-68415802\n",
      "6        /news/business-68401814\n",
      "7       /sport/football/68417181\n",
      "8    /news/world-europe-68420566\n",
      "9    /news/world-europe-68420565\n",
      "Name: Link, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Reading in the 'region_article_links.csv' into dataframe\n",
    "region_article_links = pd.read_csv('region_article_links.csv')\n",
    "print(region_article_links.head(10))\n",
    "\n",
    "#Extracting column with the links\n",
    "article_links = region_article_links['Link']\n",
    "print(article_links.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''This last cell has not been executed because the output is very large'''\n",
    "\n",
    "#Looping through the links and using the extract_article_information() function\n",
    "#on each of these appending resulting dictionary into result list\n",
    "scraped_article_info = []\n",
    "for link in article_links:\n",
    "    print(link)\n",
    "    #dismiss links in the sport section because it contains entire different layout\n",
    "    if str(link[:6]) == '/sport':\n",
    "        continue\n",
    "    else:\n",
    "        scraped_article_info.append(extract_article_information(link))\n",
    "        #added delay to avoid getting blocked\n",
    "        time.sleep(1.0)\n",
    "\n",
    "for article_info in scraped_article_info:\n",
    "    print(article_info)\n",
    "\n",
    "\n",
    "df_scraped_article_info = pd.DataFrame(scraped_article_info)\n",
    "\n",
    "#4\n",
    "# df_scraped_article_info.to_csv(\"scraped_article_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To determine whether it would make sense to include this newly aquired\n",
    "data in the dataset we can look at multiple things. First of all we saw that\n",
    "the original dataset is quite unbalanced. This could be made somewhat up for by adding all/some\n",
    "of the scraped data since they come from bbc which one could argue was a reputable source of\n",
    "information. However the size of the new dataset is much greater than the news_data sample so we \n",
    "would have to introduce some more from the full dataset as well (assuming that the full news data set also\n",
    "consists of mainly fake news articles) to balance the new concatenated dataset.\n",
    "That said, the scraped data has not been labeled at all so all of the above is just based on \n",
    "assumptions about the credibility of the bbc. Another thing that could be worth to consider doing before \n",
    "merging the two dataset is to measure the quality and reliability of the scraped data by for example \n",
    "looking at the percentage of articles with an author attribution etc. To introduce this new data into the original dataset\n",
    "there are therefore many factors to consider to avoid introducing more bias etc. into the dataset.''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
